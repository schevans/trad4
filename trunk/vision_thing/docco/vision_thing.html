<?xml version="1.0" encoding="ISO-8859-1"?>

<!-- GENERATED BY TRAD4 -->

  <!DOCTYPE html
            PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
            "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

     <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" type="text/css" href="http://trad4.sourceforge.net/gcc.css" />
<title>
vision_thing - a trad4 application
</title>
</head>

<body bgcolor="#FFFFFF" text="#000000" link="#1F00FF" alink="#FF0000" vlink="#9900DD">

<h1 align="center">
vision_thing
</h1>

<table border="1">
<tr>
<td>Application version</td>
<td>1.0</td>
</tr>
<tr>
<td>Trad4 version</td>
<td>3.1</td>
</tr>
<tr>
<td>Document version</td>
<td>1.0</td>
</tr>
<tr>
<td>Author</td>
<td>schevans</td>
</tr>
<tr>
<td>Date</td>
<td>31-12-09</td>
</tr>
</table>

<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the_model">The Model</a></li>
<li><a href="#implementation">Implementation</a></li>
<li><a href="#new_usages">New language usages</a></li>
<li><a href="#further_work">Further Work</a></li>
<li><a href="#usage">Usage</a></li>
</ul>

<hr />

<h2><a name="introduction">Introduction</a></h2>
<p>
The plan is to create a McCullogh-Pits single layer artificial neural network, show it some images of the digits 0-9 in various fonts, and see what happens. In this case we will create one neuron for each of the target images.
</p>
<h2><a name="the_model">The Model</a></h2>
<p>

</p>
<h2><a name="implementation">Implementation</a></h2>
<h3>The abstract diagram</h3>
<p>
<img src="abstract.jpg" alt="abstract"/>
</p>
<p>

</p>
<h3>The concrete diagram</h3>
<p>
<img src="concrete.jpg" alt="concrete"/>
</p>
<p>

</p>
<h3>The t4 files</h3>
<h4>input.t4:</h4>
<blockquote><pre>
sub
    monitor monitor

static
    int font_order[NUM_FONTS]

pub
    int image_number
    int font_number
    font fonts[NUM_FONTS]

</pre></blockquote>
<ul>
<li>sub:
    <ul>
    <li>monitor - the input object subscribes to the monitor so convergence of one font can be detected</li>
    </ul>
</li>

<li>static:
    <ul>
    <li>font_order[NUM_FONTS] - the order the fonts are displayed</li>
    </ul>
</li>

<li>pub:
    <ul>
    <li>image_number - the current image number being displayed</li>
    <li>font_number - the current font being displayed</li>
    <li>fonts[NUM_FONTS] - the list of fonts</li>
    </ul>
</li>

</ul>
<p>

</p>
<h4>neuron.t4:</h4>
<blockquote><pre>
sub
    input input

static
    int image

pub
    weight_matrix weights
    int output
    int correct

</pre></blockquote>
<ul>
<li>sub:
    <ul>
    <li>input - all neurons subscribe to a single input</li>
    </ul>
</li>

<li>static:
    <ul>
    <li>image - the image number which this neuron is looking for (for teaching)</li>
    </ul>
</li>

<li>pub:
    <ul>
    <li>weights - this neuron's weight matrix</li>
    <li>output - if this neuron thinks it's being shown the image it's looking for</li>
    <li>correct - whether the neuron is correct or not (found by comparing input, image and output)</li>
    </ul>
</li>

</ul>
<p>

</p>
<h4>monitor.t4:</h4>
<blockquote><pre>
sub
    input input
    neuron neurons[NUM_NEURONS]

pub
    int num_runs
    int num_cycles_correct
    int converged

</pre></blockquote>
<ul>
<li>sub:
    <ul>
    <li>input - the single input object</li>
    <li>neurons[NUM_NEURONS] - the array of neuron objects</li>
    </ul>
</li>

<li>pub:
    <ul>
    <li>num_runs - number of times all tiers have been run (used internally for detecting convergence)</li>
    <li>num_cycles_correct - the number of correct runs in a row (used internally for detecting convergence)</li>
    <li>converged - if the neurons have converged on a particular font</li>
    </ul>
</li>

</ul>
<p>
</p>
<h2><a name="new_usages">New Language usages</a></h2>
<p>
-Passing images by pointer
-3rd party libs ok
</p>
<h2><a name="further_work">Further Work</a></h2>
<p>
One notable feature about the model presented above it that it makes no difference what order the pixels are presented to the neurons - were you to scramble all the images by remapping the pixels randomly but consistently, the model will still converge. This, it turns out, is a bad thing - it means there is no account taken of the context of each pixel, which is obviously very important. More work needs to be done in this area.
</p>
<p>
The animations could be better presented and synchronised. Welcome to my life.
</p>
<p>
The neuron write weight matrix could be moved down to the neuron (I think - libgd seems to have some peculiar requirements of the heap)
</p>
<h2><a name="usage">Usage</a></h2>
<h3>Running</h3>
<p>
To run the application:
1) Download and unpack the distribution<br>
2) cd into trad4_v3_1_0/vision_thing:<br>
<blockquote><pre>$ cd trad4_v3_1_0/vision_thing</pre></blockquote>
3) Source vision_thing.conf:<br>
<blockquote><pre>vision_thing$ . ./vision_thing.conf</pre></blockquote>
4) Start vision_thing:<br>
<blockquote><pre>vision_thing$ vision_thing</pre></blockquote>
<p>
To increase or decrease the number of threads used (the default is 4), set NUM_THREADS and re-start the application:
</p>
<blockquote><pre>$ export NUM_THREADS=64
$ vision_thing</pre></blockquote>


</body>
</html>
