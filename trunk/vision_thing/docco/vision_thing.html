<?xml version="1.0" encoding="ISO-8859-1"?>

<!-- GENERATED BY TRAD4 -->

  <!DOCTYPE html
            PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
            "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

     <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" type="text/css" href="http://trad4.sourceforge.net/gcc.css" />
<title>
vision_thing - a trad4 application
</title>
</head>

<body bgcolor="#FFFFFF" text="#000000" link="#1F00FF" alink="#FF0000" vlink="#9900DD">

<h1 align="center">
vision_thing
</h1>

<table border="1">
<tr>
<td>Application version</td>
<td>1.0</td>
</tr>
<tr>
<td>Trad4 version</td>
<td>3.1</td>
</tr>
<tr>
<td>Document version</td>
<td>1.0</td>
</tr>
<tr>
<td>Author</td>
<td>schevans</td>
</tr>
<tr>
<td>Date</td>
<td>31-12-09</td>
</tr>
</table>

<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the_input">The The Input</a></li>
<li><a href="#the_neuron">The Neuron</a></li>
<li><a href="#the_model">The Model</a></li>
<li><a href="#implementation">Implementation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#new_usages">New language usages</a></li>
<li><a href="#further_work">Further Work</a></li>
<li><a href="#usage">Usage</a></li>
</ul>

<hr />

<h2><a name="introduction">Introduction</a></h2>
<p>
The plan is to create a McCullogh-Pits single layer artificial neural network, show it some images of the digits 0-9 in various fonts, and see what happens. In this case we will create one neuron for each of the target numbers.
</p>
<p>
You might want to skip straight to the results, which are <a href="./animations.html">here</a>.
</p>
<h2><a name="the_input">The Input</a></h2>
<p>
The input show to the neurons is a matrix of pixels representing the numbers 0-9 in three different fonts. For example, below are the number 1 in the three fonts.
</p>
<img src="./1.LiberationSerif-Bold.png" alt="LiberationSerif-Bold"/>
<img src="./1.Harabara.png" alt="Harabara"/>
<img src="./1.OptimusPrinceps.png" alt="OptimusPrinceps"/>
<p>
The three fonts are <a href="http://www.dafont.com/liberation-serif.font">LiberationSerif-Bold</a>, <a href="http://www.dafont.com/harabara.font">Harabara</a> and <a href="http://www.dafont.com/optimusprinceps.font">OptimusPrinceps</a>.
</p>
<p>
The numbers 0-9 in each font are shown to the network until the network converges, then the numbers in the next font are shown until the network converges on the new font etcetera until all fonts have been displayed.
</p>

<h2><a name="the_neuron">The Neuron</a></h2>
<p>
Each neuron is tasked with recognising a single digit, so in this section we'll be concentrating on the neuron who's job is it to recognise '1'.
</p>
<p>
At the heart of the neuron lies the weight matrix where each cell in the matrix corresponds to a single pixel of the input image. Each number in the matrix gives the weight or importance of a particular pixel in recognising our overall target number. We'll start with a very simple 8-bit example of the number 1: 
</p>
00111000<br>
01111000<br>
11011000<br>
00011000<br>
00011000<br>
00011000<br>
01111110<br>
01111110<br>
<p>
As you can see this is a (crude) monochrome bit map of the number 1. When this is shown to our neuron, it will multiply each 1 in the image by the corresponding value in the 8-bit weight matrix, and sum the result. If that result is greater than 0, it means it thinks it's being shown a 1 and will set it's output flag to 1.. If the result is less than or equal to 0 it meant the neuron thinks it's not a 1, and set's it's output flag to 0. It's important to note that if the neuron doesn't think it's being shown a 1, it doesn't care what it might otherwise be - that's a job for the other neurons.
</p>
<p>
Next, the neuron checks to see it it's correct. (The teaching mechanism is internal to the neurons themselves, partially to allow for concurrency but mainly because it made the most sense programatically.) There are three possible outcomes:
</p>
<p>
1) The neuron could be correct, either in correctly identifying a 1 or by correctly identifying that it's not a 1. In this case we do nothing to the weight matrix.
</p>
<p>
2) The neuron could be incorrect, by thinking it hasn't been shown a 1 when it has. In this case we need to increase the chance the next time it's show a 1 to get it correct. We do by increasing the values in the weight matrix that correspond to an active pixel. This means next time this image is shown and the results are summed we'll be above the firing threshold of 0.0.
</p>
<p>
3) The neuron could be incorrect, by thinking it has been shown a 1 where in fact it hasn't. In this case we need to decrease the chance of this happening again by decreasing the values in the weight matrix that correspond to an active pixel. This means next time the neuron will take less notice of those pixels.
</p>
<p>
This process is repeated for each of the input images until the neuron converges, convergence being defined as no adjustment to the weight matrix is made after being shown the complete set of 0-0 images.
</p>
<h2><a name="the_model">The Model</a></h2>
<p>

</p>
<h2><a name="implementation">Implementation</a></h2>
<h3>The abstract diagram</h3>
<p>
<img src="abstract.jpg" alt="abstract"/>
</p>
<p>

</p>
<h3>The concrete diagram</h3>
<p>
<img src="concrete.jpg" alt="concrete"/>
</p>
<p>

</p>
<h3>The t4 files</h3>
<h4>input.t4:</h4>
<blockquote><pre>
sub
    monitor monitor

static
    int font_order[NUM_FONTS]

pub
    int image_number
    int font_number
    font fonts[NUM_FONTS]

</pre></blockquote>
<ul>
<li>sub:
    <ul>
    <li>monitor - the input object subscribes to the monitor so convergence of one font can be detected</li>
    </ul>
</li>

<li>static:
    <ul>
    <li>font_order[NUM_FONTS] - the order the fonts are displayed</li>
    </ul>
</li>

<li>pub:
    <ul>
    <li>image_number - the current image number being displayed</li>
    <li>font_number - the current font being displayed</li>
    <li>fonts[NUM_FONTS] - the list of fonts</li>
    </ul>
</li>

</ul>
<p>

</p>
<h4>neuron.t4:</h4>
<blockquote><pre>
sub
    input input

static
    int image

pub
    weight_matrix weights
    int output
    int correct

</pre></blockquote>
<ul>
<li>sub:
    <ul>
    <li>input - all neurons subscribe to a single input</li>
    </ul>
</li>

<li>static:
    <ul>
    <li>image - the image number which this neuron is looking for (for teaching)</li>
    </ul>
</li>

<li>pub:
    <ul>
    <li>weights - this neuron's weight matrix</li>
    <li>output - if this neuron thinks it's being shown the image it's looking for</li>
    <li>correct - whether the neuron is correct or not (found by comparing input, image and output)</li>
    </ul>
</li>

</ul>
<p>

</p>
<h4>monitor.t4:</h4>
<blockquote><pre>
sub
    input input
    neuron neurons[NUM_NEURONS]

pub
    int num_runs
    int num_cycles_correct
    int converged

</pre></blockquote>
<ul>
<li>sub:
    <ul>
    <li>input - the single input object</li>
    <li>neurons[NUM_NEURONS] - the array of neuron objects</li>
    </ul>
</li>

<li>pub:
    <ul>
    <li>num_runs - number of times all tiers have been run (used internally for detecting convergence)</li>
    <li>num_cycles_correct - the number of correct runs in a row (used internally for detecting convergence)</li>
    <li>converged - if the neurons have converged on a particular font</li>
    </ul>
</li>

</ul>
<p>
</p>
<h2><a name="results">Results</a></h2>
<p>
<a  href="animations.html">Animations</a>
</p>
<h2><a name="new_usages">New Language usages</a></h2>
<p>
-Passing images by pointer
-3rd party libs ok
</p>
<h2><a name="further_work">Further Work</a></h2>
<p>
One notable feature about the model presented above it that it makes no difference what order the pixels are presented to the neurons - were you to scramble all the images by remapping the pixels randomly but consistently, the model will still converge. This, it turns out, is a bad thing - it means there is no account taken of the context of each pixel, which is obviously very important. More work needs to be done in this area.
</p>
<p>
The animations could be better presented and synchronised. Welcome to my life.
</p>
<p>
The neuron write weight matrix could be moved down to the neuron (I think - libgd seems to have some peculiar requirements of the heap)
</p>
<h2><a name="usage">Usage</a></h2>
<h3>Running</h3>
<p>
To run the application:
1) Download and unpack the distribution<br>
2) cd into trad4_v3_1_0/vision_thing:<br>
<blockquote><pre>$ cd trad4_v3_1_0/vision_thing</pre></blockquote>
3) Source vision_thing.conf:<br>
<blockquote><pre>vision_thing$ . ./vision_thing.conf</pre></blockquote>
4) Start vision_thing:<br>
<blockquote><pre>vision_thing$ vision_thing</pre></blockquote>
<p>
To increase or decrease the number of threads used (the default is 4), set NUM_THREADS and re-start the application:
</p>
<blockquote><pre>$ export NUM_THREADS=64
$ vision_thing</pre></blockquote>


</body>
</html>
