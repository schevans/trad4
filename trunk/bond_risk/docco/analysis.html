<?xml version="1.0" encoding="ISO-8859-1"?>

  <!DOCTYPE html
            PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
            "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

     <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" type="text/css" href="http://trad4.sourceforge.net/gcc.css" />
<title>
Analysis: bond_risk
</title>
</head>

<body bgcolor="#FFFFFF" text="#000000" link="#1F00FF" alink="#FF0000" vlink="#9900DD">

<h1 align="center">
Analysis: bond_risk
</h1>
<table border="1">
<tr>
<td>Download version</td>
<td><a href="">TBA</a></td>
</tr>
<tr>
<td>Download licence</td>
<td>BSD</td>
</tr>
<tr>
<td>Trad4 version</td>
<td><a href="">TBA</a></td>
</tr>
<tr>
<td>Author</td>
<td>schevans</td>
</tr>
<tr>
<td>Date</td>
<td>17-05-2009</td>
</tr>
</table>

<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#hosts">The hosts</a></li>
<li><a href="#concepts">Concepts</a></li>
<li><a href="#br440k">Test 1: 440k objects, non-optimised</a></li>
<li><a href="#br440k_opt">Test 2: 440k objects, optimised</a></li>
<li><a href="#context">Test 3: 440k objects, time vs. context switches</a></li>
<li><a href="#context_time">Test 4: 440k objects, context switches over time</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#usage">How to run the tests</a></li>
</ul>

<hr />

<h2><a name="introduction">Introduction</a></h2>
<p>
This is an analysis of the trad4 app bond_risk. This document is still in beta - if there is anything you'd like clarified or any test you'd like to see run, please let me know.
</p>
<p>
What we'll be measuring is the duration of the initial flush on start-up under various conditions. A flush is when every node gets recalculated, as happens on start-up. As such we'll be ignoring the time it takes to load the objects from the DB (a subject for later discussion). It's important to understand that this initial flush is only the start-up time of a particular app and the performace profile during day-to-day activities will be different as the system will be responding to and fedding up discrete changes.
</p>
<p>
For each test the binary is locally compiled, and the test is run with the host in single-user mode using a non-root user and the default nice.
</p>
<p>
Unless otherwise stated, all these tests were run against the 440k data set (40k bonds, 200k each or repo and outright trades).
</p>
<p>
The tests are summarised below:
</p>
<ul>
<li>Tests 1&lt;2 are comparisons between the Athlon and Opteron measuring the duration of the 440k flush duration against log2 threads using both optimised and non-optimised binaries.</li>
<li>Test 3 shows the 440k flush duration along with context switches</li>
<li>Test 4 shows context switches per second as a 440k flush runs against log2 threads.</li>
</ul>
<h2><a name="hosts"></a>The hosts</h2>
<p>
The hosts are summarised below.
</p>
<table border="1">
<tr><td>Host</td><td>Athlon</td><td>Opteron</td></tr>
<tr><td>Model</td><td>64 X2 Dual Core 4800+</td><td>Quad-Core Opteron 2354</td></tr>
<tr><td>CPU GHz*</td><td>2.4</td><td>2.1</td></tr>
<tr><td>Cores</td><td>2</td><td>8</td></tr>
<tr><td>Dies</td><td>1</td><td>2</td></tr>
<tr><td>OS Arch</td><td>i686</td><td>x86_64</td></tr>
<tr><td>Kernel</td><td>2.6.22-15-generic</td><td>2.6.27-7-server</td></tr>
<tr><td>Compiler</td><td>gcc 4.1.3</td><td>gcc 4.3.2</td></tr>
</table>
* As advertised
<p>
I will also be running a couple of these tests on a 3 year-old Intel which lacks a HyperTransport bus (QuickPath in Intel-speak), which should prove interesting.
</p>
<h2><a name="concepts">Concepts</a></h2>
<p>

</p>
<h3>Single-Threaded Mean (ST Mean)</h3>
<p>
The ST Mean value of a particular test is the mean value of 10 runs in single-threaded mode (NUM_THREADS=0). This gives us a good benchmark for the single-threaded performace of a particular arch/compiler combination.
</p>
<p>
This also gives us some information on the variance and standard deviation of these runs. This is shown in the table below:
</p>
<table border="1">
<tr><td>Test</td><td>Mean</td><td>Variance</td><td>Standard Deviation</td></tr>
<tr><td>Athlon</td><td>22.3148</td><td>0.2297</td><td>0.4792</td></tr>
<tr><td>AthlonO3</td><td>8.2094</td><td>1.3066</td><td>1.1431</td></tr>
<tr><td>Opteron</td><td>19.9114</td><td>0.2944</td><td>0.5426</td></tr>
<tr><td>OpteronO3</td><td>13.2403</td><td>0.0020</td><td>0.0450</td></tr>
</table>

<h3>Single-Threaded Mode vs. NUM_THREADS=1</h3>
<p>
Single-threaded mode means the master thread does all the work. NUM_THREADS=1 means there is one master thread and one worker thread. Single-threaded mode is faster than NUM_THREADS=1 due to the lack of overhead of handing the work off to the worker thread.
</p>
<h3>The Context-Switch Cascade</h3>
<p>
The context-switch cascade occurs when there are too many threads on a host, and the kernel spends all it's time switching between threads and not getting any work done, so the host becomes unresponsive and unusable. It is the hard limit on the number of threads we can run per host. This is given below:
</p>
<table border="1">
<tr><td>Host</td><td>Max Threads</td></tr>
<tr><td>Athlon</td><td>128</td></tr>
<tr><td>Opteron</td><td>512</td></tr>
</table>
<p>
For this reason no data is given for the Athlon where NUM_THREADS>128.
</p>
<h2><a name="br440k"></a>Test 1: 440k objects, non-optimised</h2>
<table border="1">
<tr><td>Num Threads</td><td>Athlon Time</td><td>Opteron Time</td></tr>
<tr><td>1</td><td>70.05</td><td>54.82</td></tr>
<tr><td>2</td><td>35.83</td><td>24.59</td></tr>
<tr><td>4</td><td>22.13</td><td>12.12</td></tr>
<tr><td>8</td><td>15.06</td><td>6.11</td></tr>
<tr><td>16</td><td>14.15</td><td>3.51</td></tr>
<tr><td>32</td><td>13.66</td><td>2.99</td></tr>
<tr><td>64</td><td>13.3</td><td>2.73</td></tr>
<tr><td>128</td><td>13.2</td><td>2.8</td></tr>
<tr><td>256</td><td></td><td>2.74</td></tr>
<tr><td>512</td><td></td><td>2.87</td></tr>
</table>
<p>
<img src="br440k.jpg" alt="br440k"/>
</p>
<p>
From this test we can see several things. firstly, by comparing the ST Mean of both the Athlon and Opteron we can see the single-core CPU speed is roughly comparable with the Opteron coming in as slightly faster.
</p>
<p>
Second, we can see that the multi-threaded Athlon is about twice as fast as the ST Mean. Likewise the multi-threaded Opteron is about eight times faster than the ST Mean. This we would expect as the Athlon is dual-cored and the Opteron is 8-cored. 
</p>
<p>
Last, we can see that the multi-threaded Opteron is about four times as fast as the multi-threaded Athlon. Again we would expect this as the Opteron has four-times as many cores.
</p>

<h2><a name="br440k_opt"></a>Test 2: 440k objects, optimised</h2>
<table border="1">
<tr><td>Num Cores</td><td>Athlon Time</td><td>Opteron Time</td></tr>
<tr><td>1</td><td>49.98</td><td>46.79</td></tr>
<tr><td>2</td><td>30.68</td><td>21.24</td></tr>
<tr><td>4</td><td>12.37</td><td>10.13</td></tr>
<tr><td>8</td><td>8.37</td><td>5.19</td></tr>
<tr><td>16</td><td>6.52</td><td>2.66</td></tr>
<tr><td>32</td><td>6.17</td><td>2.25</td></tr>
<tr><td>64</td><td>6.01</td><td>1.92</td></tr>
<tr><td>128</td><td>5.85</td><td>1.91</td></tr>
<tr><td>256</td><td></td><td>1.94</td></tr>
<tr><td>512</td><td></td><td>2.08</td></tr>
</table>
<p>
<img src="br440k_opt.jpg" alt="br440k_opt"/>
</p>
<p>
The first thing we notice on this graph is that the Athlon binary has improved with optimisation (-O3) significantly more than the Opteron - the Athlon's ST Mean is not only faster than the Opteron now, there's also a bigger disparity between the two ST Means. This is a somewhat unexpected result and one I'll be looking into in the future.
</p>
<p>
For this reason the multi-threaded Opteron is only about three times faster than the Athlon, when we expected a four-fold increase.
</p>
<p>
This is a somewhat unexpected result and one I'll be looking into in the future. My hypothesis is that the gcc 4.3.2 optimiser on x86_64 isn't as effective as the gcc 4.1.3 optimiser on i686. The fact the ST Means show the same pattern suggests I'm not hitting some not-yet-understood limit of the trad4 architecture.
</p>
<h2><a name="context"></a>Test 3: 440k objects, time vs. context switches</h2>
<table border="1">
<tr><td>Num Threads</td><td>Opteron Time</td><td>Opteron CS</td></tr>
<tr><td>1</td><td>46.79</td><td>105.75</td></tr>
<tr><td>2</td><td>21.24</td><td>55.69</td></tr>
<tr><td>4</td><td>10.13</td><td>30.76</td></tr>
<tr><td>8</td><td>5.19</td><td>16.94</td></tr>
<tr><td>16</td><td>2.66</td><td>11.13</td></tr>
<tr><td>32</td><td>2.25</td><td>9.2</td></tr>
<tr><td>64</td><td>1.92</td><td>8.89</td></tr>
<tr><td>128</td><td>1.91</td><td>10.01</td></tr>
<tr><td>256</td><td>1.94</td><td>13.63</td></tr>
<tr><td>512</td><td>2.08</td><td>23.78</td></tr>
</table>
<p>
<img src="br440k_context.jpg" alt="br440k_context"/>
</p>
<p>
This graph shows the duration and number of context switches against log2 num threads, using an optimised Opteron.
</p>
<p>
The reason for the high number of context switches for a low number of threads is simply because the run has a longer duration and therefore a longer context-switch sample period. This is better illustrated in the graph below.
</p>
<p>
Another observation we can make is that even after the number of context switches starts to climb, the duration continues to fall (albeit slightly). This suggests any optimisation strategy should not be an attempt to minimise the context-switches - it is only when the cascade starts will we see performance drop off.
</p>
<h2><a name="context_time"></a>Test 4: 440k objects, context switches over time</h2>
<p>
<img src="br440k_context_time.jpg" alt="br440k_context_time"/>
</p>
<p>
This data was collected from a second process collecting CS/s stats while an optimised Operon run was taking place. As such the x-axis shows real-time and each peak corresponds to a bond_risk run where the number of threads are dobled each run. 
</p>
<p>
You can now see why the NUM_THREADS=1 run in Test3 was inflated by the time it took to complete the run. The CS/run data in Test3 corresponds to the integration of each peak on this graph.
</p>
<p>
You can see the beginnings of the context switch cascade on the NUM_THREADS=1024 spike. When this occurs the process monitoring CS/s can't get enough time on the CPU to record the CS/s, so the experiment is terminated.
</p>
<h2><a name="conclusion">Conclusion</a></h2>
<h3>Intra-host</h3>
<p>
<img src="br440k_st_duration.jpg" alt="br440k_st_duration"/>
</p>
<p>

</p>
<h3>Inter-host</h3>
<p>
<img src="br440k_ath_opt.jpg" alt="br440k_ath_opt"/>
</p>
<p>


</p>
<h2><a name="usage">How to run the tests</a></h2>
<p>
To run the tests:
</p>
<ol>
<li>Download(TBA), unpack and set-up as usual, sourcing bond_risk.conf</li>
<li>Set APP_DB to point to bond_risk_440k.db</li>
<li>Recompile with -O3 in CXXFLAGS if required</li>
<li>Run benchmarker.sh, which produces a benchmark.log.$$ file in $APP_ROOT.</li>
</ol>
<p>
<b>Pro Tip:</b> Run benchmarker.sh in the foreground so that it's easy to kill once you hit the context switch cascade. If in a multi-user/desktop environment and you don't run in the foreground you may have to hard-reboot your machine.
</p>
<!-- ==================================================================== -->

<div align="center" class="copyright">
Copyright (c) Steve Evans 2009
</div>

<!-- ==================================================================== -->
<br />
<center>
<!--
<a href="http://sourceforge.net/projects/trad4"><img src="http://sflogo.sourceforge.net/sflogo.php?group_id=202177&type=16" width="150" height="40" border="0" alt="Get trad4 at SourceForge.net. Fast, secure and Free Open Source software downloads" /></a>
-->
</center>


</body>
</html>
