<?xml version="1.0" encoding="ISO-8859-1"?>

  <!DOCTYPE html
            PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
            "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

     <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" type="text/css" href="http://trad4.sourceforge.net/gcc.css" />
<title>
Analysis: bond_risk
</title>
</head>

<body bgcolor="#FFFFFF" text="#000000" link="#1F00FF" alink="#FF0000" vlink="#9900DD">

<h1 align="center">
Analysis: bond_risk
</h1>
<table border="1">
<tr>
<td>Application version</td>
<td><a href="">TBA</a></td>
</tr>
<tr>
<td>Application licence</td>
<td>BSD</td>
</tr>
<tr>
<td>Trad4 version</td>
<td><a href="">TBA</a></td>
</tr>
<tr>
<td>Document version</td>
<td>beta01</td>
</tr>
<tr>
<td>Author</td>
<td>schevans</td>
</tr>
<tr>
<td>Date</td>
<td>17-05-2009</td>
</tr>
</table>

<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#hosts">The Hosts</a></li>
<li><a href="#concepts">Concepts</a></li>
<li><a href="#br440k">Test 1: Non-optimised</a></li>
<li><a href="#br440k_opt">Test 2: Optimised</a></li>
<li><a href="#context">Test 3: Duration vs. Context Switches</a></li>
<li><a href="#context_time">Test 4: Context Switches over Time</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#usage">How to run the tests</a></li>
</ul>

<hr />

<h2><a name="introduction">Introduction</a></h2>
<p>
This is an analysis of the trad4 app bond_risk. This document is still in beta - if there is anything you'd like clarified or any test you'd like to see run, please let me know.
</p>
<p>
What we'll be measuring is the duration of the initial flush on start-up under various conditions. A flush is when every node gets recalculated, as happens on start-up. As such we'll be ignoring the time it takes to load the objects from the DB. It's important to understand that this initial flush is only the start-up time of a particular app and the performace profile during day-to-day activities will be different. During day-to-day operation the system will be responding to events that only affect a sub-set of objects, with the possible exception of date-rolls.
</p>
<p>
For each test the binary is locally compiled, and the test is run with the host in single-user mode using a non-root user and the default nice. Only one run of each test was recorded, with the exception of the ST Mean which gave us some measure of the standard deviation across all tests. 
</p>
<p>
All these tests were run against the 440k data set: 40k bonds with 200k each of repo and outright trades.
</p>
<p>
The tests are summarised below:
</p>
<ul>
<li>Test 1 is a comparison between the Athlon and Opteron measuring the duration of the 440k flush against various numbers of threads using non-optimised binaries.</li>
<li>Test 2 is a comparison between the Athlon and Opteron measuring the duration of the 440k flush against various numbers of threads using optimised binaries.</li>
<li>Test 3 shows the 440k flush duration along with context switches</li>
<li>Test 4 shows context switches per second as a 440k flush runs against various numbers of threads.</li>
</ul>
<h2><a name="hosts"></a>The Hosts</h2>
<p>
The hosts are summarised below.
</p>
<table border="1">
<tr><td>Host</td><td>Athlon</td><td>Opteron</td></tr>
<tr><td>Model</td><td>64 X2 Dual Core 4800+</td><td>Quad-Core Opteron 2354</td></tr>
<tr><td>CPU GHz*</td><td>2.4</td><td>2.1</td></tr>
<tr><td>Cores</td><td>2</td><td>8</td></tr>
<tr><td>Dies</td><td>1</td><td>2</td></tr>
<tr><td>OS Arch</td><td>i686</td><td>x86_64</td></tr>
<tr><td>Kernel</td><td>2.6.22-15-generic</td><td>2.6.27-7-server</td></tr>
<tr><td>Compiler</td><td>gcc 4.1.3</td><td>gcc 4.3.2</td></tr>
</table>
* As advertised
<h2><a name="concepts">Concepts</a></h2>
<p>

</p>
<h3>Single-Threaded Mean (ST Mean)</h3>
<p>
The ST Mean value of a particular test is the mean value of 10 runs in single-threaded mode (NUM_THREADS=0). This gives us a good benchmark for the single-threaded performace of a particular arch/compiler combination.
</p>
<p>
This also gives us some information on the variance and standard deviation of these runs. This is shown in the table below:
</p>
<table border="1">
<tr><td>Test</td><td>Mean</td><td>Variance</td><td>Standard Deviation</td></tr>
<tr><td>Athlon</td><td>22.3148</td><td>0.2297</td><td>0.4792</td></tr>
<tr><td>AthlonO3</td><td>8.2094</td><td>1.3066</td><td>1.1431</td></tr>
<tr><td>Opteron</td><td>19.9114</td><td>0.2944</td><td>0.5426</td></tr>
<tr><td>OpteronO3</td><td>13.2403</td><td>0.0020</td><td>0.0450</td></tr>
</table>

<h3>Single-Threaded Mode vs. NUM_THREADS=1</h3>
<p>
Single-threaded mode means the master thread does all the work. NUM_THREADS=1 means there is one master thread and one worker thread. Single-threaded mode is faster than NUM_THREADS=1 due to the lack of overhead of handing the work off to the worker thread.
</p>
<h3>The Context-Switch Cascade</h3>
<p>
The context-switch cascade occurs when there are too many threads on a host, and the kernel spends all it's time switching between threads and not getting any work done so the host becomes unresponsive and unusable. It is the hard limit on the number of threads we can run per host. This is given below:
</p>
<table border="1">
<tr><td>Host</td><td>Max Threads</td></tr>
<tr><td>Athlon</td><td>128</td></tr>
<tr><td>Opteron</td><td>512</td></tr>
</table>
<p>
For this reason no data is given for the Athlon where NUM_THREADS>128.
</p>
<h2><a name="br440k"></a>Test 1: Non-optimised</h2>
<table border="1">
<tr><td>Num Threads</td><td>Athlon Time</td><td>Opteron Time</td></tr>
<tr><td>1</td><td>70.05</td><td>54.82</td></tr>
<tr><td>2</td><td>35.83</td><td>24.59</td></tr>
<tr><td>4</td><td>22.13</td><td>12.12</td></tr>
<tr><td>8</td><td>15.06</td><td>6.11</td></tr>
<tr><td>16</td><td>14.15</td><td>3.51</td></tr>
<tr><td>32</td><td>13.66</td><td>2.99</td></tr>
<tr><td>64</td><td>13.3</td><td>2.73</td></tr>
<tr><td>128</td><td>13.2</td><td>2.8</td></tr>
<tr><td>256</td><td></td><td>2.74</td></tr>
<tr><td>512</td><td></td><td>2.87</td></tr>
</table>
<p>
<img src="br440k.jpg" alt="br440k"/>
</p>
<p>
From this test we can see several things. Firstly, by comparing the ST Mean of both the Athlon and Opteron we can see the single-core CPU speed is roughly comparable with the Opteron coming in as slightly faster.
</p>
<p>
Second, we can see that the multi-threaded Athlon is about twice as fast as the ST Mean. Likewise the multi-threaded Opteron is about eight times faster than the ST Mean. This we would expect as the Athlon is dual-cored and the Opteron is 8-cored. 
</p>
<p>
Last, we can see that the multi-threaded Opteron is about four times as fast as the multi-threaded Athlon. Again we would expect this as the Opteron has four-times as many cores.
</p>

<h2><a name="br440k_opt"></a>Test 2: Optimised</h2>
<table border="1">
<tr><td>Num Cores</td><td>Athlon Time</td><td>Opteron Time</td></tr>
<tr><td>1</td><td>49.98</td><td>46.79</td></tr>
<tr><td>2</td><td>30.68</td><td>21.24</td></tr>
<tr><td>4</td><td>12.37</td><td>10.13</td></tr>
<tr><td>8</td><td>8.37</td><td>5.19</td></tr>
<tr><td>16</td><td>6.52</td><td>2.66</td></tr>
<tr><td>32</td><td>6.17</td><td>2.25</td></tr>
<tr><td>64</td><td>6.01</td><td>1.92</td></tr>
<tr><td>128</td><td>5.85</td><td>1.91</td></tr>
<tr><td>256</td><td></td><td>1.94</td></tr>
<tr><td>512</td><td></td><td>2.08</td></tr>
</table>
<p>
<img src="br440k_opt.jpg" alt="br440k_opt"/>
</p>
<p>
The first thing we notice on this graph is that the Athlon binary has improved with optimisation (-O3) significantly more than the Opteron - the Athlon's ST Mean is not only faster than the Opteron now, there's also a bigger disparity between the two ST Means.
</p>
<p>
For this reason the multi-threaded Opteron is only about three times faster than the Athlon, when we expected a four-fold increase.
</p>
<p>
This is a somewhat unexpected result and one I'll be looking into in the future. My hypothesis is that the gcc 4.3.2 optimiser on x86_64 isn't as effective as the gcc 4.1.3 optimiser on i686. The fact the ST Means show the same pattern suggests I'm not hitting some not-yet-understood limit of the trad4 architecture.
</p>
<h2><a name="context"></a>Test 3: Duration vs. Context Switches</h2>
<table border="1">
<tr><td>Num Threads</td><td>Opteron Time</td><td>Opteron CS</td></tr>
<tr><td>1</td><td>46.79</td><td>105.75</td></tr>
<tr><td>2</td><td>21.24</td><td>55.69</td></tr>
<tr><td>4</td><td>10.13</td><td>30.76</td></tr>
<tr><td>8</td><td>5.19</td><td>16.94</td></tr>
<tr><td>16</td><td>2.66</td><td>11.13</td></tr>
<tr><td>32</td><td>2.25</td><td>9.2</td></tr>
<tr><td>64</td><td>1.92</td><td>8.89</td></tr>
<tr><td>128</td><td>1.91</td><td>10.01</td></tr>
<tr><td>256</td><td>1.94</td><td>13.63</td></tr>
<tr><td>512</td><td>2.08</td><td>23.78</td></tr>
</table>
<p>
<img src="br440k_context.jpg" alt="br440k_context"/>
</p>
<p>
This graph shows the duration and number of context switches against #Threads, using an optimised Opteron.
</p>
<p>
The reason for the high number of context switches for a low number of threads is simply because the run has a longer duration and therefore a longer context-switch sample period. This is better illustrated in the graph below.
</p>
<p>
Another observation we can make is that even after the number of context switches starts to climb, the duration continues to fall (albeit slightly). This suggests any optimisation strategy should not be an attempt to minimise the context-switches - it is only when the cascade starts will we see performance drop off.
</p>
<h2><a name="context_time"></a>Test 4: Context Switches over Time</h2>
<p>
<img src="br440k_context_time.jpg" alt="br440k_context_time"/>
</p>
<p>
This data was collected from a second process collecting CS/s stats while an optimised Operon run was taking place. As such the x-axis shows real-time and each peak corresponds to a bond_risk run where the number of threads are doubled each run.
</p>
<p>
You can now see why the NUM_THREADS=1 run in Test3 was inflated by the time it took to complete the run: The CS/run data in Test3 corresponds to the integration of each peak on this graph.
</p>
<p>
You can see the beginnings of the context switch cascade on the NUM_THREADS=1024 spike. When this occurs the process monitoring CS/s can't get enough time on a CPU to record the CS/s, so the experiment is terminated.
</p>
<h2><a name="conclusion">Conclusion</a></h2>
<p>
In this section we'll be slicing the data already presented to prove that a trad4 application scales linearly with multiple cores - double the number of cores and you halve the calculation time.
</p>
<p>
First we'll examine each host separately (but on the same graph), testing to see how the multi-threaded stacks up to the ST Mean given X number of cores. Second, we'll compare the run durations between the two hosts to see if we can see a similar pattern.
</p>
<h3>Intra-host</h3>
<p>
<img src="br440k_st_duration.jpg" alt="br440k_st_duration"/>
</p>
<p>
The above graph show the ratio of the ST Mean and each duration for all four test cases. This shows 'how much faster' any #Thread configuration is than the single-threaded case.
</p>
<p>
You can see, above 64 threads the Operon cases are close to eight times faster than the Opteron ST Mean. The Athlon cases above 16 threads are around two times faster than the Athlon ST Mean, as expected.
</p>
<h3>Inter-host</h3>
<p>
<img src="br440k_ath_opt.jpg" alt="br440k_ath_opt"/>
</p>
<p>
This graph shows the ratio of the Athlon/Operon durations over #Thread configuration. Looking at the non-optimised data (blue line), as the #Threads increase the Opteron starts to get the edge due to the extra headroom for threads (threadroom), until it's around 4 times faster than the Athlon.
</p>
<p>
When optimised (red line), the picture looks a little different due to what looks to be an advantage the Athlon optimised binary has over the Optron optimised binary. This is largely borne out by the difference in ST Means between the optimised and non optimised versions as discussed in Test1, but this will be further investigated.
</p>
<h2><a name="usage">How to run the tests</a></h2>
<p>
To run the tests:
</p>
<ol>
<li>Download(TBA), unpack and set-up as usual, sourcing bond_risk.conf</li>
<li>Set APP_DB to point to bond_risk_440k.db</li>
<li>Recompile with -O3 in CXXFLAGS if required</li>
<li>Run benchmarker.sh, which produces a benchmark.log.$$ file in $APP_ROOT.</li>
</ol>
<p>
<b>Pro Tip:</b> Run benchmarker.sh in the foreground so that it's easy to kill once you hit the context switch cascade. If in a multi-user/desktop environment and you don't run in the foreground you may have to hard-reboot your machine.
</p>
<!-- ==================================================================== -->

<div align="center" class="copyright">
Copyright (c) Steve Evans 2009
</div>

<!-- ==================================================================== -->
<br />
<center>
<a href="http://sourceforge.net/projects/trad4"><img src="http://sflogo.sourceforge.net/sflogo.php?group_id=202177&type=16" width="150" height="40" border="0" alt="Get trad4 at SourceForge.net. Fast, secure and Free Open Source software downloads" /></a>
</center>


</body>
</html>
